{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAQ8cJH8zWHJ",
        "outputId": "bdfe60ef-5b60-40dd-a835-56d8af711653"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekS24kRUzgB5"
      },
      "source": [
        "#https://textblob.readthedocs.io/en/dev/install.html\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import datetime\n",
        "import calendar\n",
        "import math\n",
        "import time\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gszov0e0GeJ"
      },
      "source": [
        "def clean_txt(text):\n",
        "    text = re.sub(r\"(?:\\@)\\S+\", \"\", text) #Removing @mentions\n",
        "    text = re.sub('#', '', text) #Removing '#' hash tag\n",
        "    text = re.sub('RT[\\s]+', '', text) #Removing RT\n",
        "    text = re.sub('https?:\\/\\/\\S+', '', text) #Removing hyperlink\n",
        "    text = text.strip() #Removing leading and trailing spaces \n",
        "    text = text.lower() #Make tweet text lowercase\n",
        "    return text"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CxuCSx-0IlK"
      },
      "source": [
        "def analyze_sentiment(row, tol):\n",
        "    tb = TextBlob(row.tweet)\n",
        "    row['polarity'] = tb.sentiment.polarity\n",
        "    row['subjectivity'] = tb.sentiment.subjectivity\n",
        "    if abs(row['polarity']) <= tol:\n",
        "        row['tone'] = 'Neutral'\t    \n",
        "    elif row['polarity'] < 0: \n",
        "        row['tone'] = 'Negative'\n",
        "    else:\n",
        "        row['tone'] = 'Positive'\n",
        "    return row"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbeS5h430KyF"
      },
      "source": [
        "def aggregate(df, symbol, granularity, features=None, tone_tolerance=0, one_hot_encode=False, **get_dummies_kwargs):\n",
        "    \"\"\"\n",
        "    Parameter(s):\n",
        "    -------------\n",
        "    df : DataFrame\n",
        "        A dataframe of tweet data. The dataframe is assumed to be generated by twint.\n",
        "    symbol : string\n",
        "        String representing the company's stock symbol (Ex: 'AAPL').\n",
        "    granularity : string\n",
        "        Pandas date offset frequency string. 'H' for by the hour, \"T\" for by the minute, \"S\" for by the second.\n",
        "    features : array of strings or None\n",
        "        String array containing the names of the 21 possible features.\n",
        "        ['date', 'username', 'tweet', 'replies_count', 'retweets_count',\n",
        "        'likes_count', 'subjectivity', 'polarity', 'tone', 'num_tweets',\n",
        "        'replies_sum', 'retweets_sum', 'likes_sum','subjectivity_sum', \n",
        "        'polarity_sum', 'replies_avg', 'retweets_avg', 'likes_avg', \n",
        "        'subjectivity_avg', 'polarity_avg', 'tone_most_common']\n",
        "        If None is passed in (the default), all features are used.\n",
        "    tone_tolerance : float\n",
        "        If abs(polarity score) <= tone_tolerance, label the tweet as neutral.\n",
        "    one_hot_encode : bool\n",
        "        If True, one-hot encodes any categorical features specified using Pandas' `get_dummies()` function.\n",
        "    get_dummies_kwargs : dict\n",
        "        Keyword arguments for Pandas' `get_dummies(...)` function. Only has an affect if `one_hot_encode` is True.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame grouped by the given granularity with the columns specified in features.\n",
        "    If features is the empty list, returns None.\n",
        "    \"\"\"\n",
        "\n",
        "    # https://docs.python-guide.org/writing/gotchas/\n",
        "    if features is None:\n",
        "        features = ['date', 'username', 'tweet', 'cashtags', 'replies_count', \n",
        "                    'retweets_count', 'likes_count', 'subjectivity', 'polarity',\n",
        "                    'tone', 'num_tweets','replies_sum', 'retweets_sum', 'likes_sum', \n",
        "                    'subjectivity_sum', 'polarity_sum', 'replies_avg', 'retweets_avg',\n",
        "                    'likes_avg', 'subjectivity_avg', 'polarity_avg', 'tone_most_common']\n",
        "    else:\n",
        "        if len(features) == 0:\n",
        "            return None\n",
        "        features = features[:]\n",
        "    \n",
        "    #If empty dataframe, return an empty dataframe\n",
        "    if len(df) == 0:\n",
        "      print('EMPTY CSV FILE')\n",
        "      return pd.DataFrame(columns = features)\n",
        "\n",
        "    # Only keep rows where the tweet language is in English\n",
        "    df = df.loc[df['language'] == 'en',:]\n",
        "    \n",
        "    # Drop rows that are NAN for all columns\n",
        "    keep_cols = [\"date\", \"username\", \"tweet\", \"cashtags\", \"replies_count\", \"retweets_count\", \"likes_count\"]\n",
        "    df = df.copy(deep=True).loc[:, keep_cols].dropna(how='all')\n",
        "\n",
        "    # Replace any NaN values in numerical columns with 0 and in non-numerical columns with ''\n",
        "    df['username'].fillna('', inplace=True)\n",
        "    df['tweet'].fillna('', inplace=True)\n",
        "    df['replies_count'].fillna(0, inplace=True)\n",
        "    df['retweets_count'].fillna(0, inplace=True)\n",
        "    df['likes_count'].fillna(0, inplace=True)\n",
        "\n",
        "    # Clean the tweet text\n",
        "    df['tweet'] = df['tweet'].apply(clean_txt)\n",
        "\n",
        "    # Only keep rows where the company's stock symbol is in the cleaned tweet text\n",
        "    df = df[df['tweet'].str.contains(symbol.lower())].reset_index(drop=True)\n",
        "\n",
        "    # Convert columns to correct type\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df['replies_count'] = df['replies_count'].astype(int)\n",
        "    df['retweets_count'] = df['retweets_count'].astype(int)\n",
        "    df['likes_count'] = df['likes_count'].astype(int)\n",
        "\n",
        "    #SENTIMENT ANALYSIS\n",
        "    # Create three new columns: 'Subjectivity', 'Polarity', and 'Tone' with TextBlob sentiment analysis\n",
        "    df = df.apply(lambda row: analyze_sentiment(row, tone_tolerance), axis=1)\n",
        "\n",
        "    #GROUP BY TIME\n",
        "    # Group the data based on the granularity requested + sort it\n",
        "    df.set_index('date',inplace=True) #.resample() requires the date to be the index\n",
        "    grouped_twitter = df.resample(granularity).agg({'username':list,\n",
        "                                                    'tweet':list,\n",
        "                                                    'cashtags':list,\n",
        "                                                    'replies_count':list,\n",
        "                                                    'retweets_count':list,\n",
        "                                                    'likes_count':list,\n",
        "                                                    'subjectivity':list,\n",
        "                                                    'polarity':list,\n",
        "                                                    'tone':list\n",
        "                                                   })\n",
        "    grouped_twitter.reset_index(inplace=True)\n",
        "    grouped_twitter.sort_values(by=['date'], inplace=True)\n",
        "\n",
        "    # Get features for _ per time\n",
        "    if 'num_tweets' in features:\n",
        "        grouped_twitter['num_tweets'] = [len(x) for x in grouped_twitter['tweet']]\n",
        "    if 'replies_sum' in features:\n",
        "        grouped_twitter['replies_sum'] = [sum(x) for x in grouped_twitter['replies_count']]\n",
        "    if 'retweets_sum' in features:\n",
        "        grouped_twitter['retweets_sum'] = [sum(x) for x in grouped_twitter['retweets_count']]\n",
        "    if 'likes_sum' in features:\n",
        "        grouped_twitter['likes_sum'] = [sum(x) for x in grouped_twitter['likes_count']]\n",
        "    if 'subjectivity_sum' in features:\n",
        "        grouped_twitter['subjectivity_sum'] = [sum(x) for x in grouped_twitter['subjectivity']]\n",
        "    if 'polarity_sum' in features:\n",
        "        grouped_twitter['polarity_sum'] = [sum(x) for x in grouped_twitter['polarity']]\n",
        "    if 'replies_avg' in features:\n",
        "        grouped_twitter['replies_avg'] = [np.mean(x) for x in grouped_twitter['replies_count']]\n",
        "    if 'retweets_avg' in features:\n",
        "        grouped_twitter['retweets_avg'] = [np.mean(x) for x in grouped_twitter['retweets_count']]\n",
        "    if 'likes_avg' in features:\n",
        "        grouped_twitter['likes_avg'] = [np.mean(x) for x in grouped_twitter['likes_count']]\n",
        "    if 'subjectivity_avg' in features:\n",
        "        grouped_twitter['subjectivity_avg'] = [np.mean(x) for x in grouped_twitter['subjectivity']]\n",
        "    if 'polarity_avg' in features:\n",
        "        grouped_twitter['polarity_avg'] = [np.mean(x) for x in grouped_twitter['polarity']]\n",
        "    if 'tone_most_common' in features:\n",
        "        most_common = pd.Series([max(set(lst), key=lst.count) if len(lst)>0 else 'Neutral' for lst in grouped_twitter['tone']])\n",
        "        if one_hot_encode:\n",
        "            encoded = pd.get_dummies(most_common, **get_dummies_kwargs)\n",
        "            grouped_twitter = pd.concat([grouped_twitter, encoded], axis=1)\n",
        "            features.extend(encoded.columns.tolist())\n",
        "            features.remove('tone_most_common')\n",
        "        else:\n",
        "            grouped_twitter['tone_most_common'] = most_common\n",
        "        \n",
        "    return grouped_twitter.loc[:, features]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfpNDwLz0P4Q"
      },
      "source": [
        "def combine_files(month_files):\n",
        "    dfs = []\n",
        "    for file in month_files:\n",
        "        current_df = pd.read_csv(file, compression='gzip',engine='python')\n",
        "        dfs.append(current_df)\n",
        "    final_df = pd.concat(dfs)\n",
        "    return final_df"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFeTC7Jk0miH"
      },
      "source": [
        "#os.listdir() -> list of files within a directory\n",
        "#os.path.join(path,file) ->  Combines two paths\n",
        "#os.mkdir() -> creates a new file\n",
        "#cp! \"\" copies a file to drive\n",
        "\n",
        "#Every time you run, make sure to update the stock symbol at the end of the following file path\n",
        "current_stock_symbol = 'DASH'\n",
        "stock_symbol_location = '/content/gdrive/MyDrive/ieor142_twitter/twitter_data/Dash'"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1JJocQT0ThI",
        "outputId": "a978169d-5fa4-4072-c263-8be2777ea514"
      },
      "source": [
        "final_df = pd.DataFrame(columns=['date', 'num_tweets', 'replies_sum', 'retweets_sum', 'likes_sum', 'subjectivity_sum', 'polarity_sum', 'replies_avg', 'retweets_avg', 'likes_avg', 'subjectivity_avg', 'polarity_avg', 'tone_most_common'])\n",
        "\n",
        "for year in os.listdir(stock_symbol_location):\n",
        "  print(year)\n",
        "  month_path = os.path.join(stock_symbol_location,year)\n",
        "  for month in os.listdir(month_path):\n",
        "    print(month)\n",
        "    day_path = os.path.join(stock_symbol_location,year, month)\n",
        "    for day in os.listdir(day_path):\n",
        "      print(day)\n",
        "      file_path = os.path.join(stock_symbol_location,year, month, day)\n",
        "\n",
        "      #Read in the compressed csv file for the day\n",
        "      day_df = pd.read_csv(file_path, compression='gzip',engine='python')\n",
        "        \n",
        "      #Clean + Combine (Make sure to update with corresponding stock symbol)\n",
        "      day_results = aggregate(day_df, current_stock_symbol, 'D',features=['date', 'num_tweets', 'replies_sum', 'retweets_sum', 'likes_sum', 'subjectivity_sum', 'polarity_sum', 'replies_avg', 'retweets_avg', 'likes_avg', 'subjectivity_avg', 'polarity_avg', 'tone_most_common'])\n",
        "\n",
        "      #Append to final results\n",
        "      final_df = final_df.append(day_results)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweets2021\n",
            "03\n",
            "2021-03-20-tweets-%24DASH-compressed.csv\n",
            "2021-03-27-tweets-%24DASH-compressed.csv\n",
            "2021-03-18-tweets-%24DASH-compressed.csv\n",
            "2021-03-28-tweets-%24DASH-compressed.csv\n",
            "2021-03-21-tweets-%24DASH-compressed.csv\n",
            "2021-03-17-tweets-%24DASH-compressed.csv\n",
            "2021-03-19-tweets-%24DASH-compressed.csv\n",
            "2021-03-05-tweets-%24DASH-compressed.csv\n",
            "2021-03-15-tweets-%24DASH-compressed.csv\n",
            "2021-03-29-tweets-%24DASH-compressed.csv\n",
            "2021-03-22-tweets-%24DASH-compressed.csv\n",
            "2021-03-06-tweets-%24DASH-compressed.csv\n",
            "2021-03-25-tweets-%24DASH-compressed.csv\n",
            "2021-03-02-tweets-%24DASH-compressed.csv\n",
            "2021-03-26-tweets-%24DASH-compressed.csv\n",
            "2021-03-16-tweets-%24DASH-compressed.csv\n",
            "2021-03-03-tweets-%24DASH-compressed.csv\n",
            "2021-03-14-tweets-%24DASH-compressed.csv\n",
            "2021-03-24-tweets-%24DASH-compressed.csv\n",
            "2021-03-04-tweets-%24DASH-compressed.csv\n",
            "2021-03-30-tweets-%24DASH-compressed.csv\n",
            "2021-03-07-tweets-%24DASH-compressed.csv\n",
            "2021-03-11-tweets-%24DASH-compressed.csv\n",
            "2021-03-31-tweets-%24DASH-compressed.csv\n",
            "2021-03-13-tweets-%24DASH-compressed.csv\n",
            "2021-03-01-tweets-%24DASH-compressed.csv\n",
            "2021-03-23-tweets-%24DASH-compressed.csv\n",
            "2021-03-12-tweets-%24DASH-compressed.csv\n",
            "2021-03-10-tweets-%24DASH-compressed.csv\n",
            "2021-03-08-tweets-%24DASH-compressed.csv\n",
            "2021-03-09-tweets-%24DASH-compressed.csv\n",
            "01\n",
            "2021-01-31-tweets-%24DASH-compressed.csv\n",
            "2021-01-30-tweets-%24DASH-compressed.csv\n",
            "2021-01-17-tweets-%24DASH-compressed.csv\n",
            "2021-01-24-tweets-%24DASH-compressed.csv\n",
            "2021-01-18-tweets-%24DASH-compressed.csv\n",
            "2021-01-25-tweets-%24DASH-compressed.csv\n",
            "2021-01-23-tweets-%24DASH-compressed.csv\n",
            "2021-01-16-tweets-%24DASH-compressed.csv\n",
            "2021-01-26-tweets-%24DASH-compressed.csv\n",
            "2021-01-20-tweets-%24DASH-compressed.csv\n",
            "2021-01-27-tweets-%24DASH-compressed.csv\n",
            "2021-01-19-tweets-%24DASH-compressed.csv\n",
            "2021-01-28-tweets-%24DASH-compressed.csv\n",
            "2021-01-29-tweets-%24DASH-compressed.csv\n",
            "2021-01-21-tweets-%24DASH-compressed.csv\n",
            "2021-01-22-tweets-%24DASH-compressed.csv\n",
            "2021-01-04-tweets-%24DASH-compressed.csv\n",
            "2021-01-05-tweets-%24DASH-compressed.csv\n",
            "2021-01-06-tweets-%24DASH-compressed.csv\n",
            "2021-01-07-tweets-%24DASH-compressed.csv\n",
            "2021-01-08-tweets-%24DASH-compressed.csv\n",
            "2021-01-15-tweets-%24DASH-compressed.csv\n",
            "2021-01-03-tweets-%24DASH-compressed.csv\n",
            "2021-01-14-tweets-%24DASH-compressed.csv\n",
            "2021-01-09-tweets-%24DASH-compressed.csv\n",
            "2021-01-12-tweets-%24DASH-compressed.csv\n",
            "2021-01-01-tweets-%24DASH-compressed.csv\n",
            "2021-01-13-tweets-%24DASH-compressed.csv\n",
            "2021-01-02-tweets-%24DASH-compressed.csv\n",
            "2021-01-10-tweets-%24DASH-compressed.csv\n",
            "2021-01-11-tweets-%24DASH-compressed.csv\n",
            "02\n",
            "2021-02-01-tweets-%24DASH-compressed.csv\n",
            "2021-02-07-tweets-%24DASH-compressed.csv\n",
            "2021-02-02-tweets-%24DASH-compressed.csv\n",
            "2021-02-03-tweets-%24DASH-compressed.csv\n",
            "2021-02-06-tweets-%24DASH-compressed.csv\n",
            "2021-02-04-tweets-%24DASH-compressed.csv\n",
            "2021-02-08-tweets-%24DASH-compressed.csv\n",
            "2021-02-05-tweets-%24DASH-compressed.csv\n",
            "2021-02-23-tweets-%24DASH-compressed.csv\n",
            "2021-02-24-tweets-%24DASH-compressed.csv\n",
            "2021-02-28-tweets-%24DASH-compressed.csv\n",
            "2021-02-22-tweets-%24DASH-compressed.csv\n",
            "2021-02-21-tweets-%24DASH-compressed.csv\n",
            "2021-02-27-tweets-%24DASH-compressed.csv\n",
            "2021-02-25-tweets-%24DASH-compressed.csv\n",
            "2021-02-26-tweets-%24DASH-compressed.csv\n",
            "2021-02-09-tweets-%24DASH-compressed.csv\n",
            "2021-02-17-tweets-%24DASH-compressed.csv\n",
            "2021-02-16-tweets-%24DASH-compressed.csv\n",
            "2021-02-10-tweets-%24DASH-compressed.csv\n",
            "2021-02-19-tweets-%24DASH-compressed.csv\n",
            "2021-02-20-tweets-%24DASH-compressed.csv\n",
            "2021-02-18-tweets-%24DASH-compressed.csv\n",
            "2021-02-15-tweets-%24DASH-compressed.csv\n",
            "2021-02-11-tweets-%24DASH-compressed.csv\n",
            "2021-02-14-tweets-%24DASH-compressed.csv\n",
            "2021-02-13-tweets-%24DASH-compressed.csv\n",
            "2021-02-12-tweets-%24DASH-compressed.csv\n",
            "04\n",
            "2021-04-25-tweets-%24DASH-compressed.csv\n",
            "2021-04-24-tweets-%24DASH-compressed.csv\n",
            "2021-04-23-tweets-%24DASH-compressed.csv\n",
            "2021-04-22-tweets-%24DASH-compressed.csv\n",
            "2021-04-19-tweets-%24DASH-compressed.csv\n",
            "2021-04-07-tweets-%24DASH-compressed.csv\n",
            "2021-04-18-tweets-%24DASH-compressed.csv\n",
            "2021-04-20-tweets-%24DASH-compressed.csv\n",
            "2021-04-26-tweets-%24DASH-compressed.csv\n",
            "2021-04-02-tweets-%24DASH-compressed.csv\n",
            "2021-04-06-tweets-%24DASH-compressed.csv\n",
            "2021-04-04-tweets-%24DASH-compressed.csv\n",
            "2021-04-10-tweets-%24DASH-compressed.csv\n",
            "2021-04-08-tweets-%24DASH-compressed.csv\n",
            "2021-04-21-tweets-%24DASH-compressed.csv\n",
            "2021-04-17-tweets-%24DASH-compressed.csv\n",
            "2021-04-05-tweets-%24DASH-compressed.csv\n",
            "2021-04-01-tweets-%24DASH-compressed.csv\n",
            "2021-04-27-tweets-%24DASH-compressed.csv\n",
            "2021-04-11-tweets-%24DASH-compressed.csv\n",
            "2021-04-12-tweets-%24DASH-compressed.csv\n",
            "2021-04-30-tweets-%24DASH-compressed.csv\n",
            "2021-04-03-tweets-%24DASH-compressed.csv\n",
            "2021-04-14-tweets-%24DASH-compressed.csv\n",
            "2021-04-29-tweets-%24DASH-compressed.csv\n",
            "2021-04-13-tweets-%24DASH-compressed.csv\n",
            "2021-04-16-tweets-%24DASH-compressed.csv\n",
            "2021-04-28-tweets-%24DASH-compressed.csv\n",
            "2021-04-15-tweets-%24DASH-compressed.csv\n",
            "2021-04-09-tweets-%24DASH-compressed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "18gNi3gE_a6E",
        "outputId": "64a416fb-244a-49a5-fd89-10beef60ccd3"
      },
      "source": [
        "final_df['date'] = pd.to_datetime(final_df[\"date\"])\n",
        "final_df_sorted = final_df.sort_values(by=[\"date\"])\n",
        "final_df_sorted.head()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>num_tweets</th>\n",
              "      <th>replies_sum</th>\n",
              "      <th>retweets_sum</th>\n",
              "      <th>likes_sum</th>\n",
              "      <th>subjectivity_sum</th>\n",
              "      <th>polarity_sum</th>\n",
              "      <th>replies_avg</th>\n",
              "      <th>retweets_avg</th>\n",
              "      <th>likes_avg</th>\n",
              "      <th>subjectivity_avg</th>\n",
              "      <th>polarity_avg</th>\n",
              "      <th>tone_most_common</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-01</td>\n",
              "      <td>667</td>\n",
              "      <td>752</td>\n",
              "      <td>1995</td>\n",
              "      <td>7606</td>\n",
              "      <td>212.009402</td>\n",
              "      <td>61.974469</td>\n",
              "      <td>1.127436</td>\n",
              "      <td>2.991004</td>\n",
              "      <td>11.403298</td>\n",
              "      <td>0.317855</td>\n",
              "      <td>0.092915</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-02</td>\n",
              "      <td>252</td>\n",
              "      <td>94</td>\n",
              "      <td>190</td>\n",
              "      <td>1041</td>\n",
              "      <td>72.948675</td>\n",
              "      <td>22.756153</td>\n",
              "      <td>0.373016</td>\n",
              "      <td>0.753968</td>\n",
              "      <td>4.130952</td>\n",
              "      <td>0.289479</td>\n",
              "      <td>0.090302</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-03</td>\n",
              "      <td>215</td>\n",
              "      <td>183</td>\n",
              "      <td>550</td>\n",
              "      <td>1810</td>\n",
              "      <td>69.697210</td>\n",
              "      <td>30.378971</td>\n",
              "      <td>0.851163</td>\n",
              "      <td>2.558140</td>\n",
              "      <td>8.418605</td>\n",
              "      <td>0.324173</td>\n",
              "      <td>0.141298</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-04</td>\n",
              "      <td>294</td>\n",
              "      <td>190</td>\n",
              "      <td>315</td>\n",
              "      <td>1594</td>\n",
              "      <td>84.568408</td>\n",
              "      <td>35.793351</td>\n",
              "      <td>0.646259</td>\n",
              "      <td>1.071429</td>\n",
              "      <td>5.421769</td>\n",
              "      <td>0.287648</td>\n",
              "      <td>0.121746</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-05</td>\n",
              "      <td>188</td>\n",
              "      <td>67</td>\n",
              "      <td>115</td>\n",
              "      <td>706</td>\n",
              "      <td>67.062876</td>\n",
              "      <td>25.948826</td>\n",
              "      <td>0.356383</td>\n",
              "      <td>0.611702</td>\n",
              "      <td>3.755319</td>\n",
              "      <td>0.356717</td>\n",
              "      <td>0.138026</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date num_tweets  ... polarity_avg tone_most_common\n",
              "0 2021-01-01        667  ...     0.092915          Neutral\n",
              "0 2021-01-02        252  ...     0.090302          Neutral\n",
              "0 2021-01-03        215  ...     0.141298          Neutral\n",
              "0 2021-01-04        294  ...     0.121746          Neutral\n",
              "0 2021-01-05        188  ...     0.138026         Positive\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr98OMAIBkxN"
      },
      "source": [
        "#Update with current stock symbol\n",
        "file_name = current_stock_symbol + \".csv\"\n",
        "csv_file = final_df_sorted.to_csv(file_name)\n",
        "!cp DASH.csv '/content/gdrive/MyDrive/ieor142_twitter'"
      ],
      "execution_count": 120,
      "outputs": []
    }
  ]
}